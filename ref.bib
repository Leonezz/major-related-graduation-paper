
@article{ALBERT-Lan2020,
  title = {{{ALBERT}}: A {{Lite BERT}} for {{Self}}-Supervised {{Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2020},
  month = feb,
  journal = {arXiv:1909.11942 [cs]},
  eprint = {1909.11942},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \textbackslash squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {ECC: 0001414},
  file = {F\:\\zotero\\storage\\5HYGXGK3\\Lan et al_2020_ALBERT.pdf;F\:\\zotero\\storage\\J9HBZ9PH\\1909.html}
}

@book{ALuoLa2012,
  title = {计算复杂性的现代方法},
  author = {阿罗拉},
  year = {2012},
  month = jan,
  publisher = {{世界图书出版公司}},
  abstract = {Sanjeev Arora is a professor in the department of computer science at Princeton University. He has done foundational work on probabilistically checkable proofs andapproximability of NP-hardproblems. He is the founding director of the Center for Computational Intractability, which is funded by the National Science Foundation., Boaz Barak is an assistant professor in the departmen..., (展开全部)},
  isbn = {978-7-5100-4286-7},
  keywords = {数学,物理,计算机,计算机科学,计算理论},
  file = {F\:\\zotero\\storage\\QXLU8WAC\\阿罗拉 - 2012 - 计算复杂性的现代方法.pdf}
}

@article{Ba2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.06450 [cs, stat]},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {F\:\\zotero\\storage\\SGDFJYHT\\Ba et al_2016_Layer Normalization.pdf;F\:\\zotero\\storage\\C467CLNN\\1607.html}
}

@article{BART-Lewis2019,
  title = {{{BART}}: Denoising {{Sequence}}-to-{{Sequence Pre}}-Training for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.13461 [cs, stat]},
  eprint = {1910.13461},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,finished_,Statistics - Machine Learning},
  file = {F\:\\zotero\\storage\\98UQKED5\\Lewis et al_2019_BART.pdf;F\:\\zotero\\storage\\F9UVE8YV\\1910.html}
}

@inproceedings{BERT-Devlin2019,
  title = {{{BERT}}: Pre-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: Human {{Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10/ggbwf6},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  keywords = {finished_},
  file = {F\:\\zotero\\storage\\MITA9DPB\\Devlin et al_2019_BERT.pdf}
}

@inproceedings{Bordes2013,
  title = {Translating {{Embeddings}} for {{Modeling Multi}}-Relational {{Data}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bordes, Antoine and Usunier, Nicolas and {Garcia-Duran}, Alberto and Weston, Jason and Yakhnenko, Oksana},
  year = {2013},
  pages = {9},
  abstract = {We consider the problem of embedding entities and relationships of multirelational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
  langid = {english},
  keywords = {⛔ No DOI found,finished_,知识表征},
  file = {F\:\\zotero\\storage\\87MGZXC6\\Bordes 等。 - Translating Embeddings for Modeling Multi-relation.pdf;F\:\\zotero\\storage\\F3HDEPRM\\Translating-Embeddings-for-Modeling-Multi-relational-Data.pdf}
}

@book{Boswell2011,
  title = {The Art of Readable Code},
  author = {Boswell, Dustin and Foucher, Trevor},
  year = {2011},
  series = {Theory in Practice},
  edition = {1st ed},
  publisher = {{O'Reilly}},
  address = {{Sebastopol, Calif}},
  abstract = {As programmers, we've all seen source code that's so ugly and buggy it makes our brain ache. Over the past five years, authors Dustin Boswell and Trevor Foucher have analyzed hundreds of examples of "bad code" (much of it their own) to determine why they're bad and how they could be improved. Their conclusion? You need to write code that minimizes the time it would take someone else to understand it -- even if that someone else is you. This book focuses on basic principles and practical techniques you can apply every time you write code. Using easy-to-digest code examples from different languages, each chapter dives into a different aspect of coding, and demonstrates how you can make your code easy to understand. Simplify naming, commenting, and formatting with tips that apply to every line of code; Refine your program's loops, logic, and variables to reduce complexity and confusion; Attack problems at the function level, such as reorganizing blocks of code to do one task at a time; Write effective test code that is thorough and concise, as well as readable. - Publisher},
  isbn = {978-0-596-80229-5},
  lccn = {QA76.6 .B674 2012},
  keywords = {Coding theory,Computer programming},
  annotation = {OCLC: ocn767878680},
  file = {F\:\\zotero\\storage\\R54UD43I\\Boswell 和 Foucher - 2011 - The art of readable code.pdf}
}

@book{Bryant2016,
  title = {Computer Systems: A Programmer's Perspective},
  shorttitle = {Computer Systems},
  author = {Bryant, Randal E. and O'Hallaron, David},
  year = {2016},
  edition = {Third edition, global edition},
  publisher = {{Pearson Education}},
  address = {{Boston M\"unchen}},
  isbn = {978-1-292-10176-7},
  langid = {english},
  file = {F\:\\zotero\\storage\\MTZZKNS2\\Bryant 和 O'Hallaron - 2016 - Computer systems a programmer's perspective.pdf}
}

@book{Cao,
  title = {Matrix {{Theory}}},
  author = {曹, 荣美},
  file = {F\:\\zotero\\storage\\268CQM4D\\曹 - Matrix Theory.pdf}
}

@article{Chen2021,
  title = {A {{Training}}-Free and {{Reference}}-Free {{Summarization Evaluation Metric}} via {{Centrality}}-Weighted {{Relevance}} and {{Self}}-Referenced {{Redundancy}}},
  author = {Chen, Wang and Li, Piji and King, Irwin},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.13945 [cs]},
  eprint = {2106.13945},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In recent years, reference-based and supervised summarization evaluation metrics have been widely explored. However, collecting human-annotated references and ratings are costly and time-consuming. To avoid these limitations, we propose a training-free and reference-free summarization evaluation metric. Our metric consists of a centrality-weighted relevance score and a self-referenced redundancy score. The relevance score is computed between the pseudo reference built from the source document and the given summary, where the pseudo reference content is weighted by the sentence centrality to provide importance guidance. Besides an \$F\_1\$-based relevance score, we also design an \$F\_\textbackslash beta\$-based variant that pays more attention to the recall score. As for the redundancy score of the summary, we compute a self-masked similarity score with the summary itself to evaluate the redundant information in the summary. Finally, we combine the relevance and redundancy scores to produce the final evaluation score of the given summary. Extensive experiments show that our methods can significantly outperform existing methods on both multi-document and single-document summarization evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {F\:\\zotero\\storage\\NVHYFQP4\\Chen 等。 - 2021 - A Training-free and Reference-free Summarization E.pdf;F\:\\zotero\\storage\\AGDD6I69\\2106.html}
}

@book{Cover2008,
  title = {信息论基础},
  author = {Cover, Thomas},
  year = {2008},
  series = {计算机科学丛书},
  publisher = {{机械工业出版社}},
  isbn = {978-7-111-22040-4},
  file = {F\:\\zotero\\storage\\R6UJ3MIQ\\Cover - 2008 - 信息论基础.pdf}
}

@article{Cui2019,
  title = {Pre-{{Training}} with {{Whole Word Masking}} for {{Chinese BERT}}},
  author = {Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing and Wang, Shijin and Hu, Guoping},
  year = {2019},
  month = oct,
  journal = {arXiv:1906.08101 [cs]},
  eprint = {1906.08101},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks. Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT. In this technical report, we adapt whole word masking in Chinese text, that masking the whole word instead of masking Chinese characters, which could bring another challenge in Masked Language Model (MLM) pre-training task. The proposed models are verified on various NLP tasks, across sentence-level to document-level, including machine reading comprehension (CMRC 2018, DRCD, CJRC), natural language inference (XNLI), sentiment classification (ChnSentiCorp), sentence pair matching (LCQMC, BQ Corpus), and document classification (THUCNews). Experimental results on these datasets show that the whole word masking could bring another significant gain. Moreover, we also examine the effectiveness of the Chinese pre-trained models: BERT, ERNIE, BERT-wwm, BERT-wwm-ext, RoBERTa-wwm-ext, and RoBERTa-wwm-ext-large. We release all the pre-trained models: \textbackslash url\{https://github.com/ymcui/Chinese-BERT-wwm\vphantom\}},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,masking},
  file = {F\:\\zotero\\storage\\NJRKAXKD\\Cui et al_2019_Pre-Training with Whole Word Masking for Chinese BERT.pdf;F\:\\zotero\\storage\\UK3FQNQM\\1906.html}
}

@article{Cui2021,
  title = {Template-{{Based Named Entity Recognition Using BART}}},
  author = {Cui, Leyang and Wu, Yu and Liu, Jian and Yang, Sen and Zhang, Yue},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.01760 [cs]},
  eprint = {2106.01760},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {There is a recent interest in investigating few-shot NER, where the low-resource target domain has different label sets compared with a resource-rich source domain. Existing methods use a similarity-based metric. However, they cannot make full use of knowledge transfer in NER model parameters. To address the issue, we propose a template-based method for NER, treating NER as a language model ranking problem in a sequence-to-sequence framework, where original sentences and statement templates filled by candidate named entity span are regarded as the source sequence and the target sequence, respectively. For inference, the model is required to classify each candidate span based on the corresponding template scores. Our experiments demonstrate that the proposed method achieves 92.55\% F1 score on the CoNLL03 (rich-resource task), and significantly better than fine-tuning BERT 10.88\%, 15.34\%, and 11.73\% F1 score on the MIT Movie, the MIT Restaurant, and the ATIS (low-resource task), respectively.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,reading_},
  file = {F\:\\zotero\\storage\\48TZ6FGL\\Cui et al_2021_Template-Based Named Entity Recognition Using BART.pdf;F\:\\zotero\\storage\\CNV63KZB\\2106.html}
}

@article{Dai2015,
  title = {Semi-Supervised {{Sequence Learning}}},
  author = {Dai, Andrew M. and Le, Quoc V.},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.01432 [cs]},
  eprint = {1511.01432},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {F\:\\zotero\\storage\\H87XBW7J\\Dai_Le_2015_Semi-supervised Sequence Learning.pdf;F\:\\zotero\\storage\\LAQ5RH6N\\1511.html}
}

@article{Doddapaneni2021,
  title = {A {{Primer}} on {{Pretrained Multilingual Language Models}}},
  author = {Doddapaneni, Sumanth and Ramesh, Gowtham and Kunchukuttan, Anoop and Kumar, Pratyush and Khapra, Mitesh M.},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.00676 [cs]},
  eprint = {2107.00676},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Multilingual Language Models (MLLMs) such as mBERT, XLM, XLM-R, \textbackslash textit\{etc.\} have emerged as a viable option for bringing the power of pretraining to a large number of languages. Given their success in zero shot transfer learning, there has emerged a large body of work in (i) building bigger MLLMs covering a large number of languages (ii) creating exhaustive benchmarks covering a wider variety of tasks and languages for evaluating MLLMs (iii) analysing the performance of MLLMs on monolingual, zero shot crosslingual and bilingual tasks (iv) understanding the universal language patterns (if any) learnt by MLLMs and (v) augmenting the (often) limited capacity of MLLMs to improve their performance on seen or even unseen languages. In this survey, we review the existing literature covering the above broad areas of research pertaining to MLLMs. Based on our survey, we recommend some promising directions of future research.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  annotation = {QID: Q107615497},
  file = {F\:\\zotero\\storage\\WDI3IIGA\\Doddapaneni et al_2021_A Primer on Pretrained Multilingual Language Models.pdf;F\:\\zotero\\storage\\IZCBAFU5\\2107.html}
}

@article{ELMo-Peters2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  journal = {arXiv:1802.05365 [cs]},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {F\:\\zotero\\storage\\885L62KM\\Peters et al_2018_Deep contextualized word representations.pdf;F\:\\zotero\\storage\\87X4N9GE\\1802.html}
}

@article{Ernie-Baidu-Sun2019,
  title = {{{ERNIE}}: Enhanced {{Representation}} through {{Knowledge Integration}}},
  shorttitle = {{{ERNIE}}},
  author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and Tian, Hao and Wu, Hua},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.09223 [cs]},
  eprint = {1904.09223},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,finished_,masking},
  file = {F\:\\zotero\\storage\\73ZAPF42\\Sun et al_2019_ERNIE.pdf;F\:\\zotero\\storage\\RP7PR8ZC\\ERNIE-Enhanced-Representation-through-Knowledge-Integration.pdf;F\:\\zotero\\storage\\HAA67VAA\\1904.html}
}

@article{Ernie-TH-Zhang2019,
  title = {{{ERNIE}}: Enhanced {{Language Representation}} with {{Informative Entities}}},
  shorttitle = {{{ERNIE}}},
  author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  year = {2019},
  month = jun,
  journal = {arXiv:1905.07129 [cs]},
  eprint = {1905.07129},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,finished_,masking},
  file = {F\:\\zotero\\storage\\PFKUTG6Q\\Zhang et al_2019_ERNIE.pdf;F\:\\zotero\\storage\\PZPM3GNK\\ERNIE-Enhanced-Language-Representation-with-Informative-Entities.pdf;F\:\\zotero\\storage\\NZCPTB9E\\1905.html}
}

@article{Ernie2-Sun2020,
  title = {{{ERNIE}} 2.0: A {{Continual Pre}}-{{Training Framework}} for {{Language Understanding}}},
  shorttitle = {{{ERNIE}} 2.0},
  author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {8968--8975},
  issn = {2374-3468, 2159-5399},
  doi = {10/gg7mdt},
  abstract = {Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.},
  file = {F\:\\zotero\\storage\\FCYJ4X4E\\Sun et al_2020_ERNIE 2.pdf}
}

@article{Ernie3-Sun2021,
  title = {{{ERNIE}} 3.0: Large-Scale {{Knowledge Enhanced Pre}}-Training for {{Language Understanding}} and {{Generation}}},
  shorttitle = {{{ERNIE}} 3.0},
  author = {Sun, Yu and Wang, Shuohuan and Feng, Shikun and Ding, Siyu and Pang, Chao and Shang, Junyuan and Liu, Jiaxiang and Chen, Xuyi and Zhao, Yanbin and Lu, Yuxiang and Liu, Weixin and Wu, Zhihua and Gong, Weibao and Liang, Jianzhong and Shang, Zhizhou and Sun, Peng and Liu, Wei and Ouyang, Xuan and Yu, Dianhai and Tian, Hao and Wu, Hua and Wang, Haifeng},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.02137 [cs]},
  eprint = {2107.02137},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8\% (90.6\% vs. 89.8\%).},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {F\:\\zotero\\storage\\FKTI4TQH\\Sun et al_2021_ERNIE 3.pdf;F\:\\zotero\\storage\\IVCSLJ62\\2107.html}
}

@book{Gao,
  title = {{{LeetCode101}}},
  author = {高, 畅},
  file = {F\:\\zotero\\storage\\IYP9PQPG\\高 - LeetCode101.pdf}
}

@article{Giorgi2019,
  title = {End-to-End {{Named Entity Recognition}} and {{Relation Extraction}} Using {{Pre}}-Trained {{Language Models}}},
  author = {Giorgi, John and Wang, Xindi and Sahar, Nicola and Shin, Won Young and Bader, Gary D. and Wang, Bo},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.13415 [cs]},
  eprint = {1912.13415},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Named entity recognition (NER) and relation extraction (RE) are two important tasks in information extraction and retrieval (IE \textbackslash\& IR). Recent work has demonstrated that it is beneficial to learn these tasks jointly, which avoids the propagation of error inherent in pipeline-based systems and improves performance. However, state-of-the-art joint models typically rely on external natural language processing (NLP) tools, such as dependency parsers, limiting their usefulness to domains (e.g. news) where those tools perform well. The few neural, end-to-end models that have been proposed are trained almost completely from scratch. In this paper, we propose a neural, end-to-end model for jointly extracting entities and their relations which does not rely on external NLP tools and which integrates a large, pre-trained language model. Because the bulk of our model's parameters are pre-trained and we eschew recurrence for self-attention, our model is fast to train. On 5 datasets across 3 domains, our model matches or exceeds state-of-the-art performance, sometimes by a large margin.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {F\:\\zotero\\storage\\7QS44W4V\\Giorgi 等。 - 2019 - End-to-end Named Entity Recognition and Relation E.pdf}
}

@article{Han2021,
  title = {Pre-{{Trained Models}}: Past, {{Present}} and {{Future}}},
  shorttitle = {Pre-{{Trained Models}}},
  author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
  year = {2021},
  month = aug,
  journal = {arXiv:2106.07139 [cs]},
  eprint = {2106.07139},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {F\:\\zotero\\storage\\9WUD6ZXX\\Han et al_2021_Pre-Trained Models.pdf;F\:\\zotero\\storage\\U2ACS6JP\\2106.html}
}

@inproceedings{He2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10/gdcfkn},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {F\:\\zotero\\storage\\9LHR9PA3\\He et al_2016_Deep Residual Learning for Image Recognition.pdf;F\:\\zotero\\storage\\ZPSUA59I\\7780459.html}
}

@article{Hedderich2021,
  title = {A {{Survey}} on {{Recent Approaches}} for {{Natural Language Processing}} in {{Low}}-{{Resource Scenarios}}},
  author = {Hedderich, Michael A. and Lange, Lukas and Adel, Heike and Str{\"o}tgen, Jannik and Klakow, Dietrich},
  year = {2021},
  month = apr,
  journal = {arXiv:2010.12309 [cs]},
  eprint = {2010.12309},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {F\:\\zotero\\storage\\Q57F5P67\\Hedderich et al_2021_A Survey on Recent Approaches for Natural Language Processing in Low-Resource.pdf;F\:\\zotero\\storage\\6N6CKZ8F\\2010.html}
}

@article{Karamcheti2021,
  title = {Mind {{Your Outliers}}! {{Investigating}} the {{Negative Impact}} of {{Outliers}} on {{Active Learning}} for {{Visual Question Answering}}},
  author = {Karamcheti, Siddharth and Krishna, Ranjay and {Fei-Fei}, Li and Manning, Christopher D.},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.02331 [cs]},
  eprint = {2107.02331},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers -- groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {F\:\\zotero\\storage\\5T3HMHAJ\\Karamcheti 等。 - 2021 - Mind Your Outliers! Investigating the Negative Imp.pdf;F\:\\zotero\\storage\\5DIQ68XC\\2107.html}
}

@book{KeNuTe2002,
  title = {{计算机程序设计艺术. 第一卷, 基本算法（第三版）}},
  shorttitle = {{计算机程序设计艺术}},
  author = {克努特, D.E and Knuth, Donald E and {苏运霖}},
  year = {2002},
  publisher = {{国防工业出版社}},
  address = {{北京}},
  isbn = {978-7-118-02799-0},
  langid = {chinese},
  annotation = {OCLC: 867438997},
  file = {F\:\\zotero\\storage\\XDJIEFKL\\克努特 等。 - 2002 - 计算机程序设计艺术 第3版. 第1卷, 第1卷,.pdf}
}

@book{KeNuTe2002a,
  title = {{计算机程序设计艺术. 第3卷, 排序与查找（第二版）}},
  author = {{克努特} and Knuth, Donald E and {苏运霖}},
  year = {2002},
  publisher = {{国防工业出版社}},
  address = {{北京}},
  isbn = {978-7-118-02812-6},
  langid = {chinese},
  annotation = {OCLC: 301941573},
  file = {F\:\\zotero\\storage\\7XUFSK8E\\克努特 等。 - 2002 - 计算机程序设计艺术. 第3卷, 第3卷,.pdf}
}

@book{Knuth2002,
  title = {{计算机程序设计艺术. 第2卷, 半数值算法（第三版）}},
  shorttitle = {{计算机程序设计艺术}},
  author = {{Knuth} and {苏运霖}},
  year = {2002},
  publisher = {{国防工业出版社}},
  address = {{北京}},
  isbn = {978-7-118-02707-5},
  langid = {chinese},
  annotation = {OCLC: 952720100},
  file = {F\:\\zotero\\storage\\W9ZSYMIE\\Knuth 和 苏运霖 - 2002 - 计算机程序设计艺术 (第3版) 第2卷, 第2卷,.pdf}
}

@inproceedings{li-etal-2020-rigid,
  title = {Rigid Formats Controlled Text Generation},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Li, Piji and Zhang, Haisong and Liu, Xiaojiang and Shi, Shuming},
  year = {2020},
  month = jul,
  pages = {742--751},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10/gm5vjq},
  file = {F\:\\zotero\\storage\\7LU4L8HG\\Rigid-Formats-Controlled-Text-Generation.pdf;F\:\\zotero\\storage\\9RXHQUHX\\Li et al_2020_Rigid formats controlled text generation.pdf}
}

@article{Li2021,
  title = {Tail-to-{{Tail Non}}-{{Autoregressive Sequence Prediction}} for {{Chinese Grammatical Error Correction}}},
  author = {Li, Piji and Shi, Shuming},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.01609 [cs]},
  eprint = {2106.01609},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We investigate the problem of Chinese Grammatical Error Correction (CGEC) and present a new framework named Tail-to-Tail (\textbackslash textbf\{TtT\}) non-autoregressive sequence prediction to address the deep issues hidden in CGEC. Considering that most tokens are correct and can be conveyed directly from source to target, and the error positions can be estimated and corrected based on the bidirectional context information, thus we employ a BERT-initialized Transformer Encoder as the backbone model to conduct information modeling and conveying. Considering that only relying on the same position substitution cannot handle the variable-length correction cases, various operations such substitution, deletion, insertion, and local paraphrasing are required jointly. Therefore, a Conditional Random Fields (CRF) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the token dependencies. Since most tokens are correct and easily to be predicted/conveyed to the target, then the models may suffer from a severe class imbalance issue. To alleviate this problem, focal loss penalty strategies are integrated into the loss functions. Moreover, besides the typical fix-length error correction datasets, we also construct a variable-length corpus to conduct experiments. Experimental results on standard datasets, especially on the variable-length datasets, demonstrate the effectiveness of TtT in terms of sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and Correction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,finished_},
  file = {F\:\\zotero\\storage\\8ENQAVHX\\Tail-to-Tail-Non-Autoregressive-Sequence-Prediction-for-Chinese-Grammatical-Error-Correction.pdf;F\:\\zotero\\storage\\9PKQNEYT\\Li 和 Shi - 2021 - Tail-to-Tail Non-Autoregressive Sequence Predictio.pdf;F\:\\zotero\\storage\\XSLGTYU2\\2106.html}
}

@article{Liu2021,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: A {{Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.13586 [cs]},
  eprint = {2107.13586},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,finished_},
  file = {F\:\\zotero\\storage\\R6IN99JB\\Liu et al_2021_Pre-train, Prompt, and Predict.pdf;F\:\\zotero\\storage\\DVQ47XV9\\2107.html}
}

@article{Lones2021,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  shorttitle = {How to Avoid Machine Learning Pitfalls},
  author = {Lones, Michael A.},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.02497 [cs]},
  eprint = {2108.02497},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This document gives a concise outline of some of the common mistakes that occur when using machine learning techniques, and what can be done to avoid them. It is intended primarily as a guide for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning},
  file = {F\:\\zotero\\storage\\7CTE8DZ7\\Lones_2021_How to avoid machine learning pitfalls.pdf;F\:\\zotero\\storage\\3H2LM7N2\\2108.html}
}

@article{Nguyen2020,
  title = {A Survey of Embedding Models of Entities and Relationships for Knowledge Graph Completion},
  author = {Nguyen, Dat Quoc},
  year = {2020},
  month = oct,
  journal = {arXiv:1703.08098 [cs]},
  eprint = {1703.08098},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Knowledge graphs (KGs) of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge graphs are typically incomplete, it is useful to perform knowledge graph completion or link prediction, i.e. predict whether a relationship not in the knowledge graph is likely to be true. This paper serves as a comprehensive survey of embedding models of entities and relationships for knowledge graph completion, summarizing up-to-date experimental results on standard benchmark datasets and pointing out potential future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {F\:\\zotero\\storage\\RZAB7PYF\\Nguyen_2020_A survey of embedding models of entities and relationships for knowledge graph.pdf;F\:\\zotero\\storage\\LRX8SM6Q\\1703.html}
}

@book{Petersen,
  title = {The {{Matrix Cookbook}}},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  file = {F\:\\zotero\\storage\\3N685FZ6\\Petersen 和 Pedersen - The Matrix Cookbook.pdf}
}

@book{Peterson2015,
  title = {计算机网络：系统方法（原书第5版）},
  author = {Peterson, Larry L. and Davie, Bruce S.},
  translator = {王勇 and 张龙飞 and 李明 and 薛静锋},
  year = {2015},
  month = jun,
  series = {\&nbsp;计算机科学丛书                          {{ISBN}}: 9787111499077},
  publisher = {{机械工业出版社}},
  isbn = {978-7-111-49907-7},
  keywords = {CS,教材,网络,计算机,计算机基础,计算机科学,计算机网络},
  file = {F\:\\zotero\\storage\\CGH93AM5\\Peterson 和 Davie - 2015 - 计算机网络：系统方法（原书第5版）.pdf}
}

@article{Qiu2020,
  title = {Pre-Trained {{Models}} for {{Natural Language Processing}}: A {{Survey}}},
  shorttitle = {Pre-Trained {{Models}} for {{Natural Language Processing}}},
  author = {Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  year = {2020},
  month = oct,
  journal = {Science China Technological Sciences},
  volume = {63},
  number = {10},
  eprint = {2003.08271},
  eprinttype = {arxiv},
  pages = {1872--1897},
  issn = {1674-7321, 1869-1900},
  doi = {10/ghqkn2},
  abstract = {Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy with four perspectives. Next, we describe how to adapt the knowledge of PTMs to the downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {F\:\\zotero\\storage\\GBDHFVJY\\Qiu et al_2020_Pre-trained Models for Natural Language Processing.pdf;F\:\\zotero\\storage\\JLX6C49S\\2003.html}
}

@article{radford2018improving,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  keywords = {⛔ No DOI found},
  file = {F\:\\zotero\\storage\\P7FN36QZ\\Radford et al_Improving language understanding by generative pre-training.pdf}
}

@inproceedings{RoBERTa-Liu2019,
  title = {{{RoBERTa}}: A {{Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  booktitle = {{{arXiv}}:1907.11692 [Cs]},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,finished_},
  annotation = {00000},
  file = {F\:\\zotero\\storage\\EX2SNSIG\\Liu et al_2019_RoBERTa.pdf;F\:\\zotero\\storage\\WLWQNJ4K\\1907.html}
}

@article{Schick2021,
  title = {It's {{Not Just Size That Matters}}: Small {{Language Models Are Also Few}}-{{Shot Learners}}},
  shorttitle = {It's {{Not Just Size That Matters}}},
  author = {Schick, Timo and Sch{\"u}tze, Hinrich},
  year = {2021},
  month = apr,
  journal = {arXiv:2009.07118 [cs]},
  eprint = {2009.07118},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much "greener" in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {F\:\\zotero\\storage\\VXXA9YVQ\\Schick_Schütze_2021_It's Not Just Size That Matters.pdf;F\:\\zotero\\storage\\22WPEAXC\\2009.html}
}

@inproceedings{seq2seq-Sutskever2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {F\:\\zotero\\storage\\UBUEVQ2A\\Sutskever et al_2014_Sequence to Sequence Learning with Neural Networks.pdf}
}

@article{Song2019,
  title = {{{MASS}}: Masked {{Sequence}} to {{Sequence Pre}}-Training for {{Language Generation}}},
  shorttitle = {{{MASS}}},
  author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  year = {2019},
  month = jun,
  journal = {arXiv:1905.02450 [cs]},
  eprint = {1905.02450},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {F\:\\zotero\\storage\\NPZQKATZ\\Song et al_2019_MASS.pdf;F\:\\zotero\\storage\\JQ9K5JCH\\1905.html}
}

@article{SpanBERT-Joshi2020,
  title = {{{SpanBERT}}: Improving {{Pre}}-Training by {{Representing}} and {{Predicting Spans}}},
  shorttitle = {{{SpanBERT}}},
  author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
  year = {2020},
  month = jan,
  journal = {arXiv:1907.10529 [cs]},
  eprint = {1907.10529},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-large, our single model obtains 94.6\% and 88.7\% F1 on SQuAD 1.1 and 2.0, respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6\textbackslash\% F1), strong performance on the TACRED relation extraction benchmark, and even show gains on GLUE.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,masking},
  file = {F\:\\zotero\\storage\\QH2Y8423\\Joshi et al_2020_SpanBERT.pdf;F\:\\zotero\\storage\\ZGJ7QQZL\\1907.html}
}

@article{Su2021,
  title = {Non-{{Autoregressive Text Generation}} with {{Pre}}-Trained {{Language Models}}},
  author = {Su, Yixuan and Cai, Deng and Wang, Yan and Vandyke, David and Baker, Simon and Li, Piji and Collier, Nigel},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.08220 [cs]},
  eprint = {2102.08220},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model to greatly improve performance. Additionally, we devise mechanisms to alleviate the two common problems of vanilla NAG models: the inflexibility of prefixed output length and the conditional independence of individual token predictions. Lastly, to further increase the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {F\:\\zotero\\storage\\HQBZ76AI\\Su 等。 - 2021 - Non-Autoregressive Text Generation with Pre-traine.pdf;F\:\\zotero\\storage\\E4QQ28U8\\2102.html}
}

@article{T5-Raffel2020a,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text}}-to-{{Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  month = jul,
  journal = {arXiv:1910.10683 [cs, stat]},
  eprint = {1910.10683},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {F\:\\zotero\\storage\\SHK6J374\\Raffel et al_2020_Exploring the Limits of Transfer Learning with a Unified Text-to-Text.pdf;F\:\\zotero\\storage\\N38UJA25\\1910.html}
}

@inproceedings{Transformer-Vaswani2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  langid = {english},
  keywords = {⛔ No DOI found,finished_},
  annotation = {ECC: 0020487},
  file = {F\:\\zotero\\storage\\RA7UJER6\\Vaswani et al_2017_Attention is All you Need.pdf;F\:\\zotero\\storage\\SR2YMHXG\\Attention is all you need.pdf;F\:\\zotero\\storage\\M3MQY7CT\\3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@article{Wang2021,
  title = {Generating {{Diversified Comments}} via {{Reader}}-{{Aware Topic Modeling}} and {{Saliency Detection}}},
  author = {Wang, Wei and Li, Piji and Zheng, Hai-Tao},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.06856 [cs]},
  eprint = {2102.06856},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Automatic comment generation is a special and challenging task to verify the model ability on news content comprehension and language generation. Comments not only convey salient and interesting information in news articles, but also imply various and different reader characteristics which we treat as the essential clues for diversity. However, most of the comment generation approaches only focus on saliency information extraction, while the reader-aware factors implied by comments are neglected. To address this issue, we propose a unified reader-aware topic modeling and saliency information detection framework to enhance the quality of generated comments. For reader-aware topic modeling, we design a variational generative clustering algorithm for latent semantic learning and topic mining from reader comments. For saliency information detection, we introduce Bernoulli distribution estimating on news content to select saliency information. The obtained topic representations as well as the selected saliency information are incorporated into the decoder to generate diversified and informative comments. Experimental results on three datasets show that our framework outperforms existing baseline methods in terms of both automatic metrics and human evaluation. The potential ethical issues are also discussed in detail.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {F\:\\zotero\\storage\\YZR2KA4V\\Wang 等。 - 2021 - Generating Diversified Comments via Reader-Aware T.pdf;F\:\\zotero\\storage\\P359MG62\\2102.html}
}

@article{Wang2021a,
  title = {Sentence {{Semantic Regression}} for {{Text Generation}}},
  author = {Wang, Wei and Li, Piji and Zheng, Hai-Tao},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.02984 [cs]},
  eprint = {2108.02984},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recall the classical text generation works, the generation framework can be briefly divided into two phases: \textbackslash textbf\{idea reasoning\} and \textbackslash textbf\{surface realization\}. The target of idea reasoning is to figure out the main idea which will be presented in the following talking/writing periods. Surface realization aims to arrange the most appropriate sentence to depict and convey the information distilled from the main idea. However, the current popular token-by-token text generation methods ignore this crucial process and suffer from many serious issues, such as idea/topic drift. To tackle the problems and realize this two-phase paradigm, we propose a new framework named Sentence Semantic Regression (\textbackslash textbf\{SSR\}) based on sentence-level language modeling. For idea reasoning, two architectures \textbackslash textbf\{SSR-AR\} and \textbackslash textbf\{SSR-NonAR\} are designed to conduct sentence semantic regression autoregressively (like GPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a mixed-granularity sentence decoder is designed to generate text with better consistency by jointly incorporating the predicted sentence-level main idea as well as the preceding contextual token-level information. We conduct experiments on four tasks of story ending prediction, story ending generation, dialogue generation, and sentence infilling. The results show that SSR can obtain better performance in terms of automatic metrics and human evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {F\:\\zotero\\storage\\B6VF9J6Z\\Wang 等。 - 2021 - Sentence Semantic Regression for Text Generation.pdf;F\:\\zotero\\storage\\ARKV26RR\\2108.html}
}

@article{Wang2021b,
  title = {{{CLINE}}: Contrastive {{Learning}} with {{Semantic Negative Examples}} for {{Natural Language Understanding}}},
  shorttitle = {{{CLINE}}},
  author = {Wang, Dong and Ding, Ning and Li, Piji and Zheng, Hai-Tao},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.00440 [cs]},
  eprint = {2107.00440},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations. Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics. Different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes. To study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes. To address this problem, we propose Contrastive Learning with semantIc Negative Examples (CLINE), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking. By comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations. Empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks. And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {F\:\\zotero\\storage\\MPJPUERP\\Wang 等。 - 2021 - CLINE Contrastive Learning with Semantic Negative.pdf;F\:\\zotero\\storage\\3WSL5BID\\2107.html}
}

@article{Wu2018,
  title = {A {{Survey}} of {{Techniques}} for {{Constructing Chinese Knowledge Graphs}} and {{Their Applications}}},
  author = {Wu, Tianxing and Qi, Guilin and Li, Cheng and Wang, Meng},
  year = {2018},
  month = sep,
  journal = {Sustainability},
  volume = {10},
  number = {9},
  pages = {3245},
  issn = {2071-1050},
  doi = {10/gfht89},
  abstract = {With the continuous development of intelligent technologies, knowledge graph, the backbone of artificial intelligence, has attracted much attention from both academic and industrial communities due to its powerful capability of knowledge representation and reasoning. In recent years, knowledge graph has been widely applied in different kinds of applications, such as semantic search, question answering, knowledge management and so on. Techniques for building Chinese knowledge graphs are also developing rapidly and different Chinese knowledge graphs have been constructed to support various applications. Under the background of the ``One Belt One Road (OBOR)'' initiative, cooperating with the countries along OBOR on studying knowledge graph techniques and applications will greatly promote the development of artificial intelligence. At the same time, the accumulated experience of China in developing knowledge graphs is also a good reference to develop non-English knowledge graphs. In this paper, we aim to introduce the techniques of constructing Chinese knowledge graphs and their applications, as well as analyse the impact of knowledge graph on OBOR. We first describe the background of OBOR, and then introduce the concept and development history of knowledge graph and typical Chinese knowledge graphs. Afterwards, we present the details of techniques for constructing Chinese knowledge graphs, and demonstrate several applications of Chinese knowledge graphs. Finally, we list some examples to explain the potential impacts of knowledge graph on OBOR.},
  langid = {english},
  file = {F\:\\zotero\\storage\\XFUWJZUE\\Wu et al_2018_A Survey of Techniques for Constructing Chinese Knowledge Graphs and Their.pdf}
}

@book{WuJun2012,
  title = {{Beauty of mathematics}},
  author = {{吴军}},
  year = {2012},
  publisher = {{人民邮电出版社}},
  address = {{北京}},
  abstract = {Ben shu jie shi liao xin xi he zi ran yu yan chu li yi ji ta men zai tong xin he hu lian wang ge zhong ying yong(yu yin shi bie, ji qi fan yi, sou suo, fen lei deng)zhong de shu xue yuan li.},
  isbn = {978-7-115-28282-8},
  langid = {chinese},
  keywords = {finished_},
  annotation = {OCLC: 797114030},
  file = {F\:\\zotero\\storage\\DWID5IXW\\吴军 - 2012 - 数学之美 = Beauty of mathematics.pdf}
}

@article{Xie2017,
  title = {Data {{Noising}} as {{Smoothing}} in {{Neural Network Language Models}}},
  author = {Xie, Ziang and Wang, Sida I. and Li, Jiwei and L{\'e}vy, Daniel and Nie, Aiming and Jurafsky, Dan and Ng, Andrew Y.},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.02573 [cs]},
  eprint = {1703.02573},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in \$n\$-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {F\:\\zotero\\storage\\FLNIYE7U\\Xie et al_2017_Data Noising as Smoothing in Neural Network Language Models.pdf;F\:\\zotero\\storage\\Y95M67P5\\1703.html}
}

@inproceedings{XLNet-Yang2019,
  title = {Xlnet: Generalized Autoregressive Pretraining for Language Understanding},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  year = {2019},
  volume = {32},
  keywords = {⛔ No DOI found,finished_},
  file = {F\:\\zotero\\storage\\3GB3R7UZ\\XLNet.pdf;F\:\\zotero\\storage\\GXT5GVIE\\Yang et al_2019_Xlnet.pdf}
}

@article{Xu2021,
  title = {Vocabulary {{Learning}} via {{Optimal Transport}} for {{Neural Machine Translation}}},
  author = {Xu, Jingjing and Zhou, Hao and Gan, Chun and Zheng, Zaixiang and Li, Lei},
  year = {2021},
  month = aug,
  journal = {arXiv:2012.15671 [cs]},
  eprint = {2012.15671},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The choice of token vocabulary affects the performance of machine translation. This paper aims to figure out what is a good vocabulary and whether one can find the optimal vocabulary without trial training. To answer these questions, we first provide an alternative understanding of the role of vocabulary from the perspective of information theory. Motivated by this, we formulate the quest of vocabularization -- finding the best token dictionary with a proper size -- as an optimal transport (OT) problem. We propose VOLT, a simple and efficient solution without trial training. Empirical results show that VOLT outperforms widely-used vocabularies in diverse scenarios, including WMT-14 English-German and TED's 52 translation directions. For example, VOLT achieves almost 70\% vocabulary size reduction and 0.5 BLEU gain on English-German translation. Also, compared to BPE-search, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https://github.com/Jingjing-NLP/VOLT .},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {F\:\\zotero\\storage\\IVJLFVGL\\Xu et al_2021_Vocabulary Learning via Optimal Transport for Neural Machine Translation.pdf;F\:\\zotero\\storage\\WXCBIBYN\\2012.html}
}

@article{Yin2021,
  title = {Including {{Signed Languages}} in {{Natural Language Processing}}},
  author = {Yin, Kayo and Moryossef, Amit and Hochgesang, Julie and Goldberg, Yoav and Alikhani, Malihe},
  year = {2021},
  month = jul,
  journal = {arXiv:2105.05222 [cs]},
  eprint = {2105.05222},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {F\:\\zotero\\storage\\F94SL6EL\\Yin et al_2021_Including Signed Languages in Natural Language Processing.pdf;F\:\\zotero\\storage\\2G6MLA9Z\\2105.html}
}

@article{Zhang2020,
  title = {M-{{SQL}}: Multi-{{Task Representation Learning}} for {{Single}}-{{Table Text2sql Generation}}},
  shorttitle = {M-{{SQL}}},
  author = {Zhang, Xiaoyu and Yin, Fengjing and Ma, Guojie and Ge, Bin and Xiao, Weidong},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {43156--43167},
  issn = {2169-3536},
  doi = {10/gmpvnc},
  abstract = {Text2SQL can help non-professionals connect with databases by turning natural languages into SQL. Although previous researches about Text2SQL have provided some workable solutions, most of them extract values based on column representation. If there are multiple values in the query and these values belong to different columns, the previous approaches based on column representation cannot accurately extract values. In this work, we propose a new neural network architecture based on the pre-trained BERT, called M-SQL. The column-based value extraction is divided into two modules, value extraction and value-column matching. We evaluate M-SQL on a more complicated TableQA dataset, which comes from an AI competition. We rank first in this competition. Experimental results and competition ranking show that our proposed M-SQL achieves state-of-the-art results on TableQA.},
  keywords = {Bit error rate,Databases,Decoding,M-SQL,multi-task learning,Natural languages,pre-trained,Semantics,Syntactics,Task analysis,Text2SQL},
  file = {F\:\\zotero\\storage\\PXIKVB9E\\Zhang 等。 - 2020 - M-SQL Multi-Task Representation Learning for Sing.pdf;F\:\\zotero\\storage\\TLMBUJRF\\9020099.html}
}

@book{zotero-1138,
  title = {Latex {{Mathematical Symbols Reference}}},
  file = {F\:\\zotero\\storage\\ZKSHVRVG\\Latex Mathematical Symbols Reference.pdf}
}


